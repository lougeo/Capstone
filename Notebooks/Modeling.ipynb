{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Louis George    \n",
    "\n",
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import joblib\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import chart_studio\n",
    "import chart_studio.plotly as py\n",
    "chart_studio.tools.set_credentials_file(username='lougeo', api_key=os.environ.get(\"PLOTLY_API\"))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('ytick', labelsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = pd.read_csv('../../data/X_train_tfidf.csv')\n",
    "X_test_tfidf = pd.read_csv('../../data/X_test_tfidf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_countv = pd.read_csv('../../data/X_train_countv.csv')\n",
    "X_test_countv = pd.read_csv('../../data/X_test_countv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_imdb_train = pd.read_csv('../../data/y_imdb_train.csv')\n",
    "y_rt_train = pd.read_csv('../../data/y_rt_train.csv')\n",
    "y_profit_train = pd.read_csv('../../data/y_profit_train.csv')\n",
    "\n",
    "y_imdb_test = pd.read_csv('../../data/y_imdb_test.csv')\n",
    "y_rt_test = pd.read_csv('../../data/y_rt_test.csv')\n",
    "y_profit_test = pd.read_csv('../../data/y_profit_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the possibility of using Latent Dirichlet Allocation for decomposition      \n",
    "\n",
    "Due to the differences in scale between the genre dummies, part of speech counts, and word vectors, I will use only the word vectors. Scaling is not required when modeling using the full data set with Logistic Regression and Decision Tree based models, so this won't be an issue going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_countv_vecs = X_train_countv.iloc[:, 28:]\n",
    "X_test_countv_vecs = X_test_countv.iloc[:, 28:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_num = [3, 5, 10, 20]\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for i in topic_num:\n",
    "    lda = LatentDirichletAllocation(n_components=i).fit(X_train_countv_vecs)\n",
    "    lda_train_weights = lda.transform(X_train_countv_vecs)\n",
    "    lda_test_weights = lda.transform(X_test_countv_vecs)\n",
    "    \n",
    "    logreg = LogisticRegression().fit(lda_train_weights, y_imdb_train)\n",
    "    train_scores.append(logreg.score(lda_train_weights, y_imdb_train))\n",
    "    test_scores.append(logreg.score(lda_test_weights, y_imdb_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(topic_num, train_scores, marker='.', label='train')\n",
    "plt.plot(topic_num, test_scores, marker='.', label='test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from this graph, LDA decomposition is not going to give us any good results. For fun, let's see what the topics look like for one of the more reasonable results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function which shows the features with the highest coefficients for each topic\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "# This function is courtesy of the sklearn documentation @:\n",
    "# https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic_num = 20\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=topic_num).fit(X_train_countv_vecs)\n",
    "\n",
    "#transforming for t-SNE\n",
    "lda_transformed = lda.transform(X_train_countv_vecs)\n",
    "\n",
    "n_top_words = 20\n",
    "feature_names = X_train_countv_vecs.columns\n",
    "\n",
    "print_top_words(lda, feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a lot to look at, and even if there were fewer topics it would still be difficult for me to just look at these lists of words and make any sense out of it (These are just 20 words out of the whole data set!). To simplify this task let's make some plots! Below are a 2d, and 3d tSNE plots classed using KMeans clustering:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2D t-SNE plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tSNE = TSNE(n_components=2)\n",
    "X_tSNE = my_tSNE.fit_transform(lda_transformed)\n",
    "\n",
    "k_means_model = KMeans(n_clusters = 20)\n",
    "k_means_model.fit(X_tSNE)\n",
    "pred_labels = k_means_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(pred_labels, bins=20)\n",
    "plt.title(\"Distribution of Genre Classes\")\n",
    "plt.xlabel(\"Genre Class\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the tSNE-transformed data (only the first two dimensions)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(X_tSNE[:,0],X_tSNE[:,1], c=pred_labels)\n",
    "\n",
    "plt.xlabel(\"tSNE Component 1\")\n",
    "plt.ylabel(\"tSNE Component 2\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there is some distinct clustering going on! Let's see what this looks like in 3D!\n",
    "\n",
    "#### 3D t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_train = StandardScaler().fit_transform(lda_transformed)\n",
    "\n",
    "my_tSNE_3d = TSNE(n_components=3)\n",
    "X_tSNE_3d = my_tSNE_3d.fit_transform(scaled_X_train)\n",
    "\n",
    "k_means_model_3d = KMeans(n_clusters = 20)\n",
    "k_means_model_3d.fit(X_tSNE_3d)\n",
    "pred_labels_3d = k_means_model_3d.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "                                    x=X_tSNE_3d[:,0],\n",
    "                                    y=X_tSNE_3d[:,1],\n",
    "                                    z=X_tSNE_3d[:,2],\n",
    "                                    mode='markers', \n",
    "                                    name='LDA Groups', \n",
    "                                    meta=pred_labels_3d, \n",
    "                                    marker=dict(\n",
    "                                        size=10,\n",
    "                                        color=pred_labels_3d, \n",
    "                                        colorscale='rainbow',\n",
    "                                        opacity=1),\n",
    "                                    hovertemplate='%{meta}', \n",
    "                                    )])\n",
    "\n",
    "camera = dict(eye=dict(x=1, y=2, z=0.2))\n",
    "\n",
    "fig.update_layout(scene_camera=camera)\n",
    "\n",
    "# tight layout\n",
    "fig.update_layout(margin=dict(l=0, r=0, b=0, t=30), \n",
    "                  height=600, \n",
    "                  width=600, \n",
    "                  title_text=\"t-SNE Visualization of LDA Groupings\")\n",
    "fig.show()\n",
    "\n",
    "# Loading onto cloud\n",
    "#py.iplot(fig, filename=\"tSNE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's pretty hard to make any sense of this many topics. Ideally it would have resulted in topics which could be distinguished as genres, but it isn't super obvious.    \n",
    "\n",
    "Moving forward, we'll focus on tf-idf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [['IMDb', y_imdb_train, y_imdb_test], \n",
    "           ['Rotten Tomatoes', y_rt_train, y_rt_test], \n",
    "           ['Profit Margin', y_profit_train, y_profit_test]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_modeler(targets):\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    for source, y_train, y_test in targets:\n",
    "        # Pipeline\n",
    "        estimators = [('model', LogisticRegression())]\n",
    "\n",
    "        pipe = Pipeline(estimators)\n",
    "\n",
    "        # Grid search\n",
    "        param_grid = [{'model':[LogisticRegression()],\n",
    "                       'model__penalty':['l1', 'l2'],\n",
    "                       'model__C':np.logspace(-2, 2, 5, 10)}]\n",
    "\n",
    "        grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "        logreg_best = grid.fit(X_train_tfidf, y_train)\n",
    "\n",
    "        # Taking params from best model because we will need the coefficients\n",
    "        C_val = logreg_best.best_params_['model__C']\n",
    "        penalty_val = logreg_best.best_params_['model__penalty']\n",
    "\n",
    "        # Refitting the model so that we have the normal methods available\n",
    "        logreg = LogisticRegression(C=C_val, penalty=penalty_val).fit(X_train_tfidf, y_train)\n",
    "        \n",
    "        # Pickling the model\n",
    "        joblib.dump(logreg, f'../models/{source.split()[0]}_logreg.pkl')\n",
    "        \n",
    "        # Saving the scores\n",
    "        scores.append([logreg.score(X_train_tfidf, y_train), logreg.score(X_test_tfidf, y_test)])\n",
    "\n",
    "        # Creating a dataframe to compare the coefficient values\n",
    "        lr_coefs = pd.DataFrame({'Coef':X_train_tfidf.columns,\n",
    "                                 'Value':logreg.coef_[0]})\n",
    "        lr_t10 = lr_coefs.sort_values(by='Value', ascending=False).head(10)\n",
    "\n",
    "        # Plotting the results\n",
    "        plt.figure(figsize=(12,7))\n",
    "        plt.barh(lr_t10['Coef'], abs(lr_t10['Value']))\n",
    "        plt.title(f\"Features with the Highest Coefficient \\n {source} Logistic Regression\")\n",
    "        plt.xlabel(\"Coefficient Value\")\n",
    "        plt.savefig(f\"../plots/{source.split()[0]}_logreg.png\");\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_scores = log_modeler(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_profit = joblib.load('../models/Rotten_logreg.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_profit.get_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weird results with the l1 penalty. Results in a higher test score than training for Profit. Also it's a v shit result maybe less than random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgbc_modeler(targets):\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    for source, y_train, y_test in targets:\n",
    "        \n",
    "        # Pipeline\n",
    "        estimators = [('model', XGBClassifier())]\n",
    "        pipe = Pipeline(estimators)\n",
    "        \n",
    "        # Grid search\n",
    "        param_grid = [{'model':[XGBClassifier()],\n",
    "                       'model__max_depth':[4, 5, 6, 7],\n",
    "                       'model__learning_rate':[0.001, 0.01, 0.1, 0.2, 0.3],\n",
    "                       'model__n_estimators':[50, 80, 100, 200]}]\n",
    "\n",
    "        grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "        xgbc_best = grid.fit(X_train_tfidf, y_train)\n",
    "\n",
    "        # Taking params from best model because we will need the coefficients\n",
    "        md_val = xgbc_best.best_params_['model__max_depth']\n",
    "        lr_val = xgbc_best.best_params_['model__learning_rate']\n",
    "        ne_val = xgbc_best.best_params_['model__n_estimators']\n",
    "        \n",
    "        # Refitting the model so that we have the normal methods available\n",
    "        xgbc = XGBClassifier(max_depth=md_val, \n",
    "                             learning_rate=lr_val, \n",
    "                             n_estimator=ne_val).fit(X_train_tfidf, y_train)\n",
    "\n",
    "        # Pickling the model\n",
    "        joblib.dump(xgbc, f'../models/{source.split()[0]}_xgbc.pkl')\n",
    "        \n",
    "        # Saving the scores\n",
    "        scores.append([xgbc.score(X_train_tfidf, y_train), xgbc.score(X_test_tfidf, y_test)])\n",
    "\n",
    "        # Creating a dataframe to compare the coefficient values\n",
    "        xg_coefs = pd.DataFrame({'Coef':X_train_tfidf.columns,\n",
    "                                 'Value':xgbc.feature_importances_})\n",
    "        xg_t10 = xg_coefs.sort_values(by='Value', ascending=False).head(10)\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(12,7))\n",
    "        plt.barh(xg_t10['Coef'], abs(xg_t10['Value']))\n",
    "        plt.title(f\"Features with the Highest Gain \\n {source} XG Boost\")\n",
    "        plt.xlabel(\"Gain\")\n",
    "        plt.savefig(f\"../plots/{source.split()[0]}_xgbc.png\");\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc_scores = xgbc_modeler(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
