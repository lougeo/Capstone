{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Louis George    \n",
    "\n",
    "## Cleaning, and EDA of Scraped Script and Score Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMSDb Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/scripts_upto_all.csv', index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['scripts'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking some of these titles, the link to the script loads a pdf, or other document type. The script that I currently have can't handle such documents. Time allowing I may revisit this.     \n",
    "\n",
    "Now because of the way my script grabbed the genres, I also grabbed a table with links to all 18 of the various genres. This table always got read before the script genres, and so by taking everything after the first 18 I should be left with each scripts respective genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df.shape[0]):\n",
    "    df['genres'][i] = df['genres'][i].split()\n",
    "    df['genres'][i] = df['genres'][i][18:]\n",
    "    for j in range(len(df['genres'][i])):\n",
    "        temp = re.findall('[A-Za-z]', df['genres'][i][j])\n",
    "        word = ''.join(temp)\n",
    "        df['genres'][i][j] = word\n",
    "    df['genres'][i] = '|'.join(df['genres'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now drop all 65 of the movies which I wasn't able to obtain the script for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna().reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to change format of all titles with the form: \"title, The\", and \"Title: sub title\"    \n",
    "\n",
    "Turns out it handles the vast majority of the form: \"Title: sub title\" properly, and that only a handful don't go through. For that reason I am going to let them go, as there doesn't seem to be an immediatly obvious solution to capture the ones that don't go through and leave the ones that do alone (Some titles rely on both the title and sub title).   \n",
    "\n",
    "I guess that I could do it dynamically when querying: try the whole whole title, and if not correct result try with the partial title. Will do this if time permits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df.shape[0]):\n",
    "    if re.search(\"The$\", df['titles'][i]):\n",
    "        n_title = \"The \" + re.split(\", \", df['titles'][i])[0]\n",
    "        df['titles'][i] = n_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to save the titles as a csv for use to cross reference against the OMDb API and get the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['titles'].to_csv('../../data/movie_titles.csv', header='titles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OMDB API Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First set, and initial investigation as I wait for the second set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores1 = pd.read_csv('../../data/movie_info1.csv').drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores1.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = json.loads(df_scores1['info'][9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function which gets all of the target data out of the json:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(df):\n",
    "    df['IMDb_score'], df['RT_score'], df['Meta_score'], df['box_office'], df['ID'] = np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "    for i in range(df.shape[0]):\n",
    "        temp = json.loads(df['info'][i])\n",
    "        try:\n",
    "            for j in temp['Ratings']:\n",
    "                if j['Source'] == 'Internet Movie Database':\n",
    "                    df['IMDb_score'][i] = j['Value']\n",
    "                elif j['Source'] == 'Rotten Tomatoes':\n",
    "                    df['RT_score'][i] = j['Value']\n",
    "                elif j['Source'] == 'Metacritic':\n",
    "                    df['Meta_score'][i] = j['Value']\n",
    "            df['box_office'][i] = temp['BoxOffice']\n",
    "            df['ID'][i] = temp['imdbID']\n",
    "        except:\n",
    "            #print(\"Exception triggered\")\n",
    "            df['IMDb_score'][i] = np.nan\n",
    "            df['RT_score'][i] = np.nan\n",
    "            df['Meta_score'][i] = np.nan\n",
    "            df['box_office'][i] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_scores(df_scores1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores1[df_scores1['RT_score'].isna()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function which cleans up the score features:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_cleaner(df, col):\n",
    "    for i in range(df.shape[0]):\n",
    "        try:\n",
    "            if col[i] == 'N/A':\n",
    "                col[i] = np.nan\n",
    "            elif '/' in col[i]:\n",
    "                y = col[i].split('/')\n",
    "                col[i] = round(float(y[0]) / float(y[1]), 2)\n",
    "            elif '%' in col[i]:\n",
    "                col[i] = round(int(col[i].split('%')[0]) / 100, 2)\n",
    "            elif (\"$\" in col[i]) | (\"estimated\" in col[i]):\n",
    "                col[i] = int(''.join(re.findall('[0-9]', col[i])))\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_cleaner(df_scores1, df_scores1['IMDb_score'])\n",
    "score_cleaner(df_scores1, df_scores1['RT_score'])\n",
    "score_cleaner(df_scores1, df_scores1['Meta_score'])\n",
    "score_cleaner(df_scores1, df_scores1['box_office'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________________________________\n",
    "**Second set:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores2 = pd.read_csv('../../data/movie_info2.csv').drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_scores(df_scores2)\n",
    "score_cleaner(df_scores2, df_scores2['IMDb_score'])\n",
    "score_cleaner(df_scores2, df_scores2['RT_score'])\n",
    "score_cleaner(df_scores2, df_scores2['Meta_score'])\n",
    "score_cleaner(df_scores2, df_scores2['box_office'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores2.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________________\n",
    "\n",
    "Judging from the results of the omdbapi scraping, the box office revenue, and Metacritic score are unusable, and I will have to drop those columns entirely.    \n",
    "\n",
    "I will have to drop about 10% of the remainder of my rows in order to use Rotten Tomatoes, and IMDb scores.    \n",
    "\n",
    "Overall this isn't ideal, as my dataset is already relatively small. If time permits, I may revisit the scraper and try to optimize more.\n",
    "\n",
    "__________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these figures it would appear as though we need to drop box office, however we are given the imdb movie ID, which will allow me to scrape their website relatively easily (ie sans Selenium). I'll do that and see if I can get enough data to work with.\n",
    "\n",
    "For that I need to export the IDs for the scraper to do it's thing. To do this, as well as proceed with the project anyway I will need to join the two dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores1 = df_scores1.drop(['info', 'Meta_score', 'box_office'], axis=1)\n",
    "df_scores2 = df_scores2.drop(['info', 'Meta_score', 'box_office'], axis=1)\n",
    "df_scores = pd.concat([df_scores1, df_scores2], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting the movie ids for IMDb scraper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores['ID'].to_csv('../web_scraper/imbd_ids.csv', header='ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________    \n",
    "## IMDb Data\n",
    "Importing the scraped data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imdb = pd.read_csv('../../data/imdb_scrape.csv', index_col='Unnamed: 0').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imdb['info'][0].split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores['Gross_world'], df_scores['Gross_US'], df_scores['Opening_US'], df_scores['Budget'] = np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "for i in range(df_imdb.shape[0]):\n",
    "    if type(df_imdb['info'][i]) == float:\n",
    "        pass\n",
    "    else:\n",
    "        for j in df_imdb['info'][i].split('\\n'):\n",
    "            if 'Budget' in j:\n",
    "                df_scores['Budget'][i] = j\n",
    "            if 'Opening' in j:\n",
    "                df_scores['Opening_US'][i] = j\n",
    "            if 'Gross USA' in j:\n",
    "                df_scores['Gross_US'][i] = j\n",
    "            if 'Cumulative' in j:\n",
    "                df_scores['Gross_world'][i] = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imdb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imdb.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores[df_scores['Budget'].isna()].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores[df_scores['Budget'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"With 'Opening_US' dropped: {df_scores.drop(['titles', 'ID', 'Opening_US'], axis=1).dropna().shape}\")\n",
    "print(f\"With 'Opening_US', and 'Gross_US' dropped: {df_scores.drop(['titles', 'ID', 'Opening_US', 'Gross_US'], axis=1).dropna().shape}\")\n",
    "print(f\"With 'Opening_US', and 'Gross_world' dropped: {df_scores.drop(['titles', 'ID', 'Opening_US', 'Gross_world'], axis=1).dropna().shape}\")\n",
    "print(f\"With all 3 dropped: {df_scores.drop(['titles', 'ID', 'Opening_US', 'Gross_US', 'Gross_world'], axis=1).dropna().shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results of our scrape of IMDb, I will have to widdle down my dataset from 1145 scripts to either as high as 948, or as low as 898, depending on which targets I decide to include. Moving forward I will take the middle ground of 922, and drop both the Opening US, and Gross US tagerts, and continue with IMDb score, RT score, Budget, and Gross world. Once I have a completed pipeline I may revisit this decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = df_scores.drop(['ID', 'Opening_US', 'Gross_US'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_cleaner(df_scores, df_scores['Budget'])\n",
    "score_cleaner(df_scores, df_scores['Gross_world'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_scores.columns[1:]:\n",
    "    plt.figure()\n",
    "    plt.hist(df_scores[i])\n",
    "    plt.title(i)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the final dataframes in order to drop the nans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat([df_scores, df.drop('titles', axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fc = df_combined.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fc.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown we are left with the expected number of rows after dropping all nulls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to export a clean dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fc.to_csv('../../data/df_clean.csv', columns=df_fc.columns, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
