{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Louis George    \n",
    "\n",
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Warning: Lots of the functions in this notebook take a LONG time to run!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/df_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, -2:]\n",
    "y = df.iloc[:, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummying the genres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat((X, X['genres'].str.get_dummies()), axis=1).drop('genres', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the preprocessing, and tokenizing function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of numbers in the scripts from the formatting, so I want to remove those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_preprocessor(string):\n",
    "    no_d = ''.join([i for i in string if not i.isdigit()])\n",
    "    return no_d.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will remove stopwords, punctuation, and some other unwanted things in the tokenizing function, as well as lemmatize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer(string):\n",
    "    # Initializing the spacy class\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(string)\n",
    "    # List to append accepted tokens to\n",
    "    tokens = []\n",
    "    # Condition for a good token\n",
    "    for token in doc:\n",
    "\n",
    "        if (token.is_stop == False) & \\\n",
    "           (token.is_punct == False) & \\\n",
    "           (token.is_space == False) & \\\n",
    "           ('\\n' not in str(token)):\n",
    "            \n",
    "            tokens.append(token.lemma_)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating some additional features       \n",
    "Counting the part of speech    \n",
    "Warning: Takes a LONG time to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing columns\n",
    "X['Num_NOUN'], X['Num_PRON'], X['Num_PROPN'], X['Num_ADJ'], X['Num_VERB'], X['Num_ADV'] = 0, 0, 0, 0, 0, 0\n",
    "\n",
    "# Iterating over all scripts\n",
    "for i in range(X.shape[0]):\n",
    "    \n",
    "    # Initializing the spacy class\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(X['scripts'][i])\n",
    "\n",
    "    # Condition for a good token\n",
    "    for token in doc:\n",
    "        if (token.is_stop == False) & \\\n",
    "           (token.is_punct == False) & \\\n",
    "           (token.is_space == False) & \\\n",
    "           ('\\n' not in str(token)):\n",
    "            pos = token.pos_\n",
    "            # Condition for each POS\n",
    "            if pos == 'NOUN':\n",
    "                X['Num_NOUN'][i] += 1\n",
    "            elif pos == 'PRON':\n",
    "                X['Num_PRON'][i] += 1\n",
    "            elif pos == 'PROPN':\n",
    "                X['Num_PROPN'][i] += 1\n",
    "            elif pos == 'ADJ':\n",
    "                X['Num_ADJ'][i] += 1\n",
    "            elif pos == 'VERB':\n",
    "                X['Num_VERB'][i] += 1\n",
    "            elif pos == 'ADV':\n",
    "                X['Num_ADV'][i] += 1\n",
    "    print(f'Just finished: {i}', end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script takes a long time to run, so I will export a copy of the dataframe from this point, and then load it and then reload it for the remainder of the steps. I'm doing this because I will be tuning the vectorizer, and don't want to have to rerun this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv('../../data/X_plus.csv', columns=X.columns, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineering the targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a new target as the percent profit of the film.    \n",
    "This will be defined as:   \n",
    "Percent Profit = Cumulative Worldwide Gross / Budget * 100   \n",
    "Answer rounded to the nearest hundreth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y['Per_Profit'] = round((y['Gross_world'] / y['Budget'] * 100), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe combine the imdb and rt scores into a score ratio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(y.IMDb_score)\n",
    "plt.title(\"Distribution of IMDb Votes\")\n",
    "plt.xlabel(\"Vote\")\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"../plots/imdb_hist.png\");\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(y.RT_score)\n",
    "plt.title(\"Distribution of Rotten Tomatoe Votes\")\n",
    "plt.xlabel(\"Vote\")\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"../plots/rt_hist.png\");\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(y.Budget, bins=50)\n",
    "plt.show();\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(y.Gross_world, bins=50)\n",
    "plt.show();\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(y.Per_Profit, bins=50)\n",
    "plt.title(\"Distribution of Profit Margins\")\n",
    "plt.xlabel(\"Profit Margins\")\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"../plots/profit_hist.png\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMDb Score:**    \n",
    "Because the distribution is a nice and normal looking, I will categorize this variable at 0.70, which is right around both the mean and median.    \n",
    "\n",
    "**RT Score:**    \n",
    "It's interesting that this distribution is differenct from IMDbs, and gives me hope that there may be some insights to be gained here. Because this does not look like a normal distribution I'm going to split this at a more arbitrary value. Although RT classifies anything better than 60% as \"fresh\", we can see that there is a definite spike around 0.80, and the median score is 0.74. For this reason I will try a cutoff of 0.80, and reevaluate if necessary.    \n",
    "\n",
    "**Budget:**    \n",
    "Not sure if I'll classify this or not yet, might do regression for this and gross rev\n",
    "\n",
    "**Cumulative Gross Worldwide:**\n",
    "Same as above.\n",
    "\n",
    "**Percent Profit:**\n",
    "Accoding to an article published in Gizmodo, they give a general rule of thumb stating that a movie must make twice it's budget in order to break even. I will therefore select 200% as the cutoff for the Percent Profit target variable. This also is close to the median, which is about 260%.      \n",
    "\n",
    "https://io9.gizmodo.com/how-much-money-does-a-movie-need-to-make-to-be-profitab-5747305"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the raw data to reference later\n",
    "y.to_csv('../../data/y_wt.csv', columns=y.columns, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb_converter(score):\n",
    "    if score < 0.70:\n",
    "        new_score = 0\n",
    "    else:\n",
    "        new_score = 1\n",
    "    return new_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rt_converter(score):\n",
    "    if score < 0.80:\n",
    "        new_score = 0\n",
    "    else:\n",
    "        new_score = 1\n",
    "    return new_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profit_converter(score):\n",
    "    if score < 200:\n",
    "        new_score = 0\n",
    "    else:\n",
    "        new_score = 1\n",
    "    return new_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y['IMDb_score'] = y['IMDb_score'].map(imdb_converter)\n",
    "y['RT_score'] = y['RT_score'].map(rt_converter)\n",
    "y['Per_Profit'] = y['Per_Profit'].map(profit_converter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline case for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"IMDb baseline: {round(y['IMDb_score'].mean(), 3)}\")\n",
    "print(f\"RT baseline: {round(y['RT_score'].mean(), 3)}\")\n",
    "print(f\"Percent Profit baseline: {round(y['Per_Profit'].mean(), 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the targets\n",
    "y.drop('titles', axis=1).to_csv('../../data/y.csv', columns=y.drop('titles', axis=1).columns, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_n = pd.read_csv('../../data/X_plus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_n, y, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing the scripts using TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING This step takes a long time to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=0.1, \n",
    "                        max_df=0.9, \n",
    "                        preprocessor=my_preprocessor, \n",
    "                        tokenizer=my_tokenizer, \n",
    "                        ngram_range=(1,3)).fit(X_train['scripts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf_transformed = tfidf.transform(X_train['scripts'])\n",
    "X_test_tfidf_transformed = tfidf.transform(X_test['scripts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf_df = pd.DataFrame(columns=tfidf.get_feature_names(), data=X_train_tfidf_transformed.toarray())\n",
    "X_test_tfidf_df = pd.DataFrame(columns=tfidf.get_feature_names(), data=X_test_tfidf_transformed.toarray())\n",
    "\n",
    "X_train_tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf_df.sum().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf_f = pd.concat([X_train.drop('scripts', axis=1).reset_index(drop=True), X_train_tfidf_df], axis=1)\n",
    "X_test_tfidf_f = pd.concat([X_test.drop('scripts', axis=1).reset_index(drop=True), X_test_tfidf_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing using count vectorizer    \n",
    "Primarily for use with LDA - probably won't be used as its own metric.     \n",
    "Warning: This step also takes a long time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagofwords = CountVectorizer(min_df=0.1, \n",
    "                             max_df=0.9, \n",
    "                             preprocessor=my_preprocessor, \n",
    "                             tokenizer=my_tokenizer, \n",
    "                             ngram_range=(1,3)).fit(X_train['scripts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_countv_transformed = bagofwords.transform(X_train['scripts'])\n",
    "X_test_countv_transformed = bagofwords.transform(X_test['scripts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_countv_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_countv_df = pd.DataFrame(columns=bagofwords.get_feature_names(), data=X_train_countv_transformed.toarray())\n",
    "X_test_countv_df = pd.DataFrame(columns=bagofwords.get_feature_names(), data=X_test_countv_transformed.toarray())\n",
    "\n",
    "X_train_countv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_countv_df.sum().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_countv_f = pd.concat([X_train.drop('scripts', axis=1).reset_index(drop=True), X_train_countv_df], axis=1)\n",
    "X_test_countv_f = pd.concat([X_test.drop('scripts', axis=1).reset_index(drop=True), X_test_countv_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting the targets and features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf_f.to_csv('../../data/X_train_tfidf.csv', columns=X_train_tfidf_f.columns, index=False)\n",
    "X_test_tfidf_f.to_csv('../../data/X_test_tfidf.csv', columns=X_test_tfidf_f.columns, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COUNT V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_countv_f.to_csv('../../data/X_train_countv.csv', columns=X_train_countv_f.columns, index=False)\n",
    "X_test_countv_f.to_csv('../../data/X_test_countv.csv', columns=X_test_countv_f.columns, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolating the targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_imdb_train = y_train.iloc[:, 1]\n",
    "y_rt_train = y_train.iloc[:, 2]\n",
    "y_profit_train = y_train.iloc[:, -1]\n",
    "\n",
    "y_imdb_test = y_test.iloc[:, 1]\n",
    "y_rt_test = y_test.iloc[:, 2]\n",
    "y_profit_test = y_test.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting the targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_imdb_train.to_csv('../../data/y_imdb_train.csv', header=True, index=False)\n",
    "y_rt_train.to_csv('../../data/y_rt_train.csv', header=True, index=False)\n",
    "y_profit_train.to_csv('../../data/y_profit_train.csv', header=True, index=False)\n",
    "\n",
    "y_imdb_test.to_csv('../../data/y_imdb_test.csv', header=True, index=False)\n",
    "y_rt_test.to_csv('../../data/y_rt_test.csv', header=True, index=False)\n",
    "y_profit_test.to_csv('../../data/y_profit_test.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
